<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>成果及论文 - 实验室网站</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
    <!-- 头部区域 -->
    <header class="header">
        <div class="header-content">
            <div class="logo">
                <img src="images/index/logo.png" alt="实验室标志">
                <h1>刘娜实验组</h1>
            </div>
            <div class="mission">
                <p></p>
            </div>
        </div>
    </header>


    <!-- 导航栏 -->
    <nav class="navbar">
        <div class="nav-toggle">
            <i class="fas fa-bars"></i>
        </div>
        <ul class="nav-links">
            <li><a href="index.html">首页</a></li>
            <li><a href="publications.html">成果及论文</a></li>
            <li><a href="members.html">组员介绍</a></li>
            <li><a href="activities.html">课题组新闻</a></li>
        </ul>
    </nav>

    <!-- 主要内容区域 -->
    <main>
        <section class="section">
            <h2>成果及论文</h2>


            <div class="publication-year">
                <h3>已发表论文</h3>
                <div class="publication-item">
                    <div class="publication-header">
                        <span class="publication-type">期刊论文</span>
                        <span class="publication-date">2025-03-10</span>
                    </div>
                    <div class="publication-content">
                        <h4>基于特性分流的多模态对话情绪感知算法</h4>
                        <div class="authors">任钦泽，袁 野，傅柯婷，付军秀，徐 康，刘 娜</div>
                        <div class="journal">第42卷 第6期</div>
                        <div class="publication-abstract">
                            <p>在主动健康领域，多模态情绪感知技术对于监测个人健康和提供医疗陪护具有重要意义。然而，当前
                                多模态对话情绪感知技术在融合不同模态信息时面临挑战，尤其是在捕捉模态间的局部关系方面。为此，本文
                                提出了一种基于特性分流的多模态融合算法 MEPAD(Multimodal Emotion Perception Algorithm with Feature
                                Diversion)。该算法利用图神经网络捕捉对话的全局信息，并引入超复数数系和成对特征融合机制，分别提取
                                多模态数据的同质性与特异性特征。实验结果表明，MEPAD 在 IEMOCAP 和 MOSEI 数据集上的多模态对话
                                情绪感知任务中显著优于现有方法，证明了其在处理复杂情感数据方面的有效性和潜力。该研究为多模态情
                                绪感知技术在主动健康领域的应用提供了新的思路。</p>
                        </div>
                        <div class="publication-links">
                            <a href="https://www.arocmag.cn/abs/2024.12.0466" class="read-more">查看全文</a>
                            <a href="https://www.arocmag.cn/abs/2024.12.0466" class="read-more">下载PDF</a>
                        </div>
                    </div>
                </div>

            </div>

            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date">2025-04-17</span>
                </div>
                <div class="publication-content">
                    <h4>隐式情绪导向的语音驱动仿生机器人说话方法</h4>
                    <div class="authors">徐 康，袁 野，付军秀，傅柯婷，任钦泽，刘 娜</div>
                    <div class="journal">第42卷 第8期</div>
                    <div class="publication-abstract">
                        <p>本研究提出了一种创新的隐式情绪导向语音驱动方法，用于仿生机器人的面部表情与头部姿态生成。
                            该方法基于深度学习，通过引入颈部舵机控制系数，突破了传统方法仅依赖预编程随机动作序列的局限，实现
                            了音频信号到自然表情的精确映射。此外，本研究提出了一种隐式情绪导向特征融合自编码器框架，无需显式
                            输入情绪参数，即可从音频中隐含地推断情绪特征，并生成丰富的面部表情和颈部运动。实验表明，该方法在
                            多个数据集上显著优于现有技术，并且通过轻量化设计，能够高效适应资源有限的移动设备。</p>
                    </div>
                    <div class="publication-links">
                        <a href="https://www.arocmag.cn/abs/2025.01.0011" class="read-more">查看全文</a>
                        <a href="https://www.arocmag.cn/abs/2025.01.0011" class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>


            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date"></span>
                </div>
                <div class="publication-content">
                    <h4>基于运动注意力的行为识别</h4>
                    <div class="authors">吴宝磊，姚东昊，袁野</div>
                    <div class="journal"></div>
                    <div class="publication-abstract">
                        <p>动作识别是视频理解任务的关键技术，实现对动态场景的深度理解与信息挖掘，广泛应用于人机交互、虚拟现实、视频监控等领域。尽管深度模型的不断发展成熟，当前动作识别的准确性、鲁棒性和实时性得到显著提升，但是动态复杂背景仍然是动作识别技术面临的巨大挑战。为了解决该问题，本文提出了一种运动线索提取策略以及运动注意力机制，并将其融入到
                            Slowfast 基准网络以提高复杂背景下动作识别效果。具体地，运动线索提
                            取策略在基准网络的 Fast 路径发挥提取运动线索的作用，而运动注意力机制则将运动线索信息与 Slow 路径中的空间特征进行融合。上述方法在 Sth-Sth
                            V1、UCF101、HMDB51 数据集上的大量实验结果表明，运动线索提取策略以及运动注意力机制使得基准网络的动作识别性能（Top-1）分别提升 3.8%、2.8%和
                            3.1%，从而验证了该方法的有效性。
                            关键词：Slowfast；动作识别；注意力机制；图像处理；视频理解；深度学习；计算机视觉；</p>
                    </div>
                    <div class="publication-links">
                        <a href="http://qikan66.xsbja.com/index.php/Show/cid/53/aid/139414" class="read-more">查看全文</a>
                        <a href="http://qikan66.xsbja.com/index.php/Show/cid/53/aid/139414" class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>



            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date">2025-03-21</span>
                </div>
                <div class="publication-content">
                    <h4>Temporal-Spatial Redundancy Reduction in Video Sequences:
                        A Motion-Based Entropy-Driven Attention Approach</h4>
                    <div class="authors">Ye Yuan, Baolei Wu, Zifan Mo, Weiye Liu, Ji Hong, Zongdao Li, Jian Liu, Na Liu
                    </div>
                    <div class="journal">Volume 42, Issue 8</div>
                    <div class="publication-abstract">
                        <p>The existence of redundant video frames results in a substantial waste of computational
                            resources during video-understanding tasks. Frame sampling is a crucial technique in
                            improving resource utilization. However, existing sampling strategies typically adopt
                            fixed-frame selection, which lacks flexibility in handling different action categories. In
                            this paper, inspired by the neural mechanism of the human visual pathway, we propose an
                            effective and interpretable frame-sampling method called Entropy-Guided Motion
                            Enhancement Sampling (EGMESampler), which can remove redundant spatio-temporal information
                            in videos. Our fundamental motivation is that motion information is an important signal that
                            drives us to adaptively select frames from videos. Thus, we first perfor motion modeling in
                            EGMESampler to extract motion information from irrelevant backgrounds. Then, we design an
                            entropy-based dynamic sampling strategy based on motion information to ensure that the
                            sampled frames can cover important information in videos. Fi ally, we perform attention
                            operations on the motion information and sampled frames to enhance the motion expression of
                            the sampled frames and remove redundant spatial background information. Our EGMESampler can
                            be embedded in existing video processing algorithms, and experiments on five benchmark
                            datasets demonstrate its effectiveness compared to previous fixed-sampling strategies, as
                            well as its generalizability across different video models and datasets.
                        </p>
                    </div>
                    <div class="publication-links">
                        <a href="https://www.mdpi.com/journal/biomimetics" class="read-more">查看全文</a>
                        <a href="https://www.mdpi.com/journal/biomimetics" class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>


            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date">2024-09-26</span>
                </div>
                <div class="publication-content">
                    <h4>SignBot:一种基于轻量深度神经网络的人形手语机器人</h4>
                    <div class="authors">李新潮，袁 野，刘 娜</div>
                    <div class="journal">第42卷 第8期</div>
                    <div class="publication-abstract">
                        <p>人形机器人凭借高度逼真的人类外观与灵活稳定的运动能力，能够深度参与到人类社会活动场景；人机交互是人形机
                            器人进入现实世界，成为人们生活、学习和工作中的伴侣的必要能力.然而，现有机器人交互技术主要面向正常人群，缺乏适
                            用于听力或者语言障碍人士的人机交互方案.为此，设计了一种轻量级的手语识别深度网络（SignNet），该模型能够快速集成
                            到机器人硬件平台，使其具备手语识别能力.此外，本文将手语识别模型与现有表情控制算法、语音合成算法联合，使机器人
                            能够识别手语，并通过表情和语音反馈进行交互.实验结果表明，SignNet 在多个手语数据集（WLASL、CSL、SLR500）上识
                            别准确率最高达到 98.2%，进一步测试显示，该机器人能精准识别手语并提供适当反馈，为听力或语言障碍人士提供了有效
                            的人机交互途径.</p>
                    </div>
                    <div class="publication-links">
                        <a href="https://kns.cnki.net/kcms2/article/abstract?v=EKYfHJ8l29jbJmQ2DYCmPXl8-dCy0gCEDa3yFGE1PJwtrQaiU6pckvyUd_u9_BaBCtII7jhwcGHTd8_8VZDqsGsiPOgjbal0MIiaBZIBJn-IrNpbLsN5OUOF5X8iMwEFLC3R1D0npCppUUSyMwXfQYNAs7p1yceL0xZQcyTJEA_aOxZ7D_GrvA==&uniplatform=NZKPT&language=CHS"
                            class="read-more">查看全文</a>
                        <a href="https://kns.cnki.net/kcms2/article/abstract?v=EKYfHJ8l29jbJmQ2DYCmPXl8-dCy0gCEDa3yFGE1PJwtrQaiU6pckvyUd_u9_BaBCtII7jhwcGHTd8_8VZDqsGsiPOgjbal0MIiaBZIBJn-IrNpbLsN5OUOF5X8iMwEFLC3R1D0npCppUUSyMwXfQYNAs7p1yceL0xZQcyTJEA_aOxZ7D_GrvA==&uniplatform=NZKPT&language=CHS"
                            class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>


            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date">2025-03-01</span>
                </div>
                <div class="publication-content">
                    <h4>A lightweight network-based sign language robot with facial mirroring and
                        speech system</h4>
                    <div class="authors">Na Liu, Xinchao Li, Baolei Wu, Qi Yu, Lihong Wan, Tao Fang, Jianwei
                        Zhang,Qingdu Li, Ye Yuan</div>
                    <div class="journal"></div>
                    <div class="publication-abstract">
                        <p>Human–robot interaction is an essential capability for humanoid robots to enter the physical
                            world and
                            become companions in people’s lives, learning, and work. While the majority of current
                            research focuses on
                            the voice-based interactions of robots, yet over 60% of communication occurs through
                            nonverbal behaviors,
                            such as facial expressions and hand gestures. Endowing robots with the ability to
                            communicate through
                            nonverbal behavior not only enhances the interactive experience with robots but also
                            provides a potential
                            communication tool for individuals with hearing or speech impairments. Here, we develop a
                            humanoid robot
                            capable of adjusting facial movements by driving servos, and design a novel framework for
                            the robot to
                            integrate sign language recognition and facial landmark detection algorithms. This framework
                            facilitates the
                            robot recognize sign language and translate it into spoken language, while also imitating
                            the facial expressions
                            of the signers. To achieve this, we also propose a lightweight deep learning network called
                            RealTimeSignNet
                            for real-time sign language recognition. Leveraging lightweight 3D convolution modules and
                            time-dependent
                            constraints, this model adapts to various time scales, ensuring efficient processing of sign
                            language recognition
                            tasks. Experimental results demonstrate the outstanding performance of the RealTimeSignNet
                            model on
                            mainstream sign language datasets, achieving an accuracy of 88.1% on the large continuous
                            sign language
                            dataset (continuous SLR), 98.2% on the isolated sign language dataset (SLR 500), and 91.50%
                            on the English
                            sign language dataset (WLAS). The overall assessment demonstrates that our humanoid robot is
                            capable
                            of recognizing sign language and translating it into spoken language, while imitating the
                            facial emotions,
                            providing a comprehensive solution to the communication challenges faced by individuals with
                            hearing and
                            speech impairments.</p>
                    </div>
                    <div class="publication-links">
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417424023595"
                            class="read-more">查看全文</a>
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417424023595"
                            class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>


            <div class="publication-item">
                <div class="publication-header">
                    <span class="publication-type">期刊论文</span>
                    <span class="publication-date">2025-03-05</span>
                </div>
                <div class="publication-content">
                    <h4>动态场景下基于图像分割与融合点线特征的视觉SLAM方法</h4>
                    <div class="authors">姚东昊，袁野，吴宝磊，刘娜</div>
                    <div class="journal"></div>
                    <div class="publication-abstract">
                        <p>针对视觉SLAM在动态场景下定位精度差和鲁棒性不足的问题，提出一种适用于动态场景的视觉SLAM算法SPL-SLAM。该算法首先利用YOLOv8-seg实例分割网络来检测先验区域，用于动态区域识别和语义地图构建；其次，通过引入线特征约束来增强系统的鲁棒性；再次，利用运动一致性检测和角度检测算法识别动态区域，并过滤动态点特征和线特征；最后，设计了一种自适应权重的误差函数并结合剩余静态点线特征进行位姿估计，同时创建了一个静态稠密点云地图，以避免动态物体干扰。使用TUM数据集及真实场景进行实验验证，结果表明，相较于ORBSLAM3及其他相关的动态SLAM算法，SPL-SLAM算法在动态场景下具有更好的定位精度。
                        </p>
                    </div>
                    <div class="publication-links">
                        <a href="https://www.rjdk.org.cn/homeNav?lang=zh" class="read-more">查看全文</a>
                        <a href="https://kns-cnki-net-443.webvpn2.usst.edu.cn/kcms2/article/abstract?v=EKYfHJ8l29ir7_wpX_LCq4HfAl21KHwK77G8TRdwRpqOYro3qEScDJvQoavpma_1X55ai9RmBq6vX0DNvjM9iSVGllRa-6t1jarqHMs90NDgwn4zMJmjqzJ6RGlmrwo08sSkNLWMECcY5oTFT5tBreniThkyh3yeC02RSydI7-I=&uniplatform=NZKPT"
                            class="read-more">下载PDF</a>
                    </div>
                </div>
            </div>


        </section>
    </main>

    <!-- 页脚 -->
    <footer class="footer">
        <div class="footer-content">
             <div class="footer-info">
                <p> 联系方式：18155555555</p>
                <p> 地址：上海市军工路580号</p>
            </div>
            <!-- <div class="footer-links">
                <a href="#">隐私政策</a>
                <a href="#">使用条款</a>
            </div> -->
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>